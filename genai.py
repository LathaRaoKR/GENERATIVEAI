# -*- coding: utf-8 -*-
"""GENAI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bkyUzP_r1pv-3mFF21MvPqt62hoM-9Ek
"""

!pip install transformers accelerate -q

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import torch
model_name = "tiiuae/falcon-rw-1b"  # example
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

def chat(prompt):
    # Tokenize input
    inputs = tokenizer(prompt, return_tensors="pt").to(device)

    # Generate response
    output = model.generate(
        **inputs,
        max_new_tokens=100,         # limit length to avoid hanging
        do_sample=True,
        top_k=50,
        top_p=0.95,
        temperature=0.7,
        pad_token_id=tokenizer.eos_token_id
    )

    # Decode and clean up response
    response = tokenizer.decode(output[0], skip_special_tokens=True)

    # Remove the prompt text from the response for cleaner output
    return response.replace(prompt, "").strip()

qstn="what is black hole?"
response=chat(qstn)
print("Bot:",response)

qstn="what is html?"
response=chat(qstn)
print("Bot:",response)

!pip install flask flask-cors pyngrok transformers accelerate torch

from flask import Flask, request, jsonify
from flask_cors import CORS
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from pyngrok import ngrok

app = Flask(__name__)
CORS(app)

# Load Falcon model (or any model you prefer)
model_name = "tiiuae/falcon-7b-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name, torch_dtype=torch.float16, device_map="auto"
)

def chat(prompt):
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    output = model.generate(
        **inputs,
        max_new_tokens=100,
        do_sample=True,
        top_k=50,
        top_p=0.95,
        temperature=0.7,
        pad_token_id=tokenizer.eos_token_id
    )
    response = tokenizer.decode(output[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()

@app.route("/api/chat", methods=["POST"])
def chat_endpoint():
    data = request.get_json()
    prompt = data.get("prompt", "")
    response = chat(prompt)
    return jsonify({"response": response})

HF_TOKEN="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"

# Run Flask on port 5000
public_url = ngrok.connect(5000).public_url
print(f"ðŸš€ Public URL: {public_url}/api/chat")

app.run(port=5000)

